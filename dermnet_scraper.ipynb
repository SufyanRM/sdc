{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dermnet_scraper.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gEYZpgUnn_eh","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h8UT4Do-kG77","colab_type":"code","colab":{}},"source":["\"\"\"Scrapes dermnet for all images.\n","\"\"\"\n","\n","from __future__ import division\n","from __future__ import print_function\n","from __future__ import absolute_import\n","\n","import os\n","import re\n","from urllib.request import urlopen\n","from urllib.parse import urljoin\n","import pickle as cPickle\n","from bs4 import BeautifulSoup\n","\n","\n","DERMNET_PIC_PAGE = \"http://www.dermnet.com/dermatology-pictures-skin-disease-pictures/\"\n","DERMNET_HOME_PAGE = \"http://www.dermnet.com/\"\n","\n","\n","def genClass2URL():\n","    \"\"\"Create a dictionary from each DermNet class to a URL.\n","    \n","    @return image_dict: dictionary containing image urls for 23 skin disease classes.\n","    \"\"\"\n","\n","    # open DermNet root directory and get class links\n","    soup = soupify(DERMNET_PIC_PAGE)\n","    class_links = soup.find(\"table\").find_all(\"a\")\n","    n_links = len(class_links)\n","    print(\"Found {} total image classes.\".format(n_links))\n","    n_total = 0\n","\n","    img_dict = {}\n","    for i, link in enumerate(class_links):\n","        abs_link = urljoin(DERMNET_HOME_PAGE, link.get('href'))\n","        class_name = re.sub(r'[^a-z0-9A-Z\\s]+', '', link.string)\n","        print('\\nFetching URLs for class [{}/{}]: {}'.format(i + 1, n_links, class_name))\n","        # add to final dictionary {class_name: list of image links}\n","        class_images = genClassImages(abs_link)\n","        n_total += len(class_images)\n","        print('Fetched {} images. Total of {} images.'.format(len(class_images), n_total))\n","        img_dict[class_name] = class_images\n","\n","    return img_dict\n","\n","\n","def genClassImages(class_url):\n","    \"\"\"Fetch list of class images\n","    @arg class_url: web url\n","    @returns class_images: list of images\n","    \"\"\"\n","    images = []\n","    urls = genClassCategories(class_url)\n","    print('- Found {} total sub-classes for class.'.format(len(urls)))\n","\n","    for i, url in enumerate(urls):\n","        print('-- Fetching images from sub-class [{}/{}]'.format(i + 1, len(urls)))\n","        images.extend(genCategoryImages(url))\n","    \n","    return images\n","\n","\n","def genClassCategories(class_url):\n","    \"\"\"Fetch list of categories for a single class\n","    @arg class_url: web url\n","    @returns categories: list of categories\n","    \"\"\"\n","    soup = soupify(class_url)\n","    links = soup.find(\"table\").find_all(\"a\")\n","    \n","    categories = []\n","    for link in links:\n","        abs_link = urljoin(DERMNET_HOME_PAGE, link.get('href'))\n","        categories.append(abs_link)\n","    return categories\n","\n","\n","def genCategoryImages(cat_url):\n","    \"\"\"Fetches all category image urls within a series of paginated links.\n","    \n","    @arg url: a category web address.\n","    @return images: A list containing image urls.\n","    \"\"\"\n","    images = []\n","    genPageImages(cat_url, images)\n","    \n","    thumb_urls = genCategoryLinks(cat_url)    \n","    # more pages in category, add images from those thumbnail pages\n","    for page in thumb_urls:\n","        genPageImages(page, images)\n","    \n","    return images\n","\n","\n","def genCategoryLinks(url):\n","    \"\"\"Returns paginated links associated to a category, if any.\n","    \n","    @url: a category web address.\n","    @returns thumb_urls: A list of paginated link addresses.\n","    \"\"\"\n","    soup = soupify(url)\n","    pages = soup.find(\"div\", \"pagination\")\n","    thumb_urls = []\n","    \n","    if pages:  #there are multiple pages for this category\n","        for page in pages:\n","            if page.name == 'a' and page.string != 'Next':\n","                page_url = urljoin(DERMNET_HOME_PAGE, page['href'])\n","                thumb_urls.append(page_url)\n","    \n","    return thumb_urls\n","\n","\n","def genPageImages(url, image_list):\n","    \"\"\"Finds all image links in a webpage and adds them to the image list.\n","    \n","    @arg url: web url; str\n","    @arg image_list: a list of image urls.\n","                     this will be modified in place.\n","    @return None\n","    \"\"\"\n","    soup = soupify(url)\n","    thumbnails = soup.find_all(\"div\",\"thumbnails\")\n","    if thumbnails: ## there are thumbnails actually on the page\n","        for thumb in thumbnails:\n","            thumb_link = thumb.img['src']\n","            #use full image link instead of thumbnail link\n","            image_link = re.sub(r'Thumb',\"\",thumb_link)\n","            image_list.append(image_link)\n","\n","\n","def soupify(url):\n","    \"\"\"Call BeautifulSoup on a webpage\n","\n","    @arg url: web url; str\n","    @return soup: BeautifulSoup instane\n","    \"\"\"\n","    html = urlopen(url)\n","    soup = BeautifulSoup(html, \"lxml\")\n","    return soup\n","\n","\n","if __name__ == '__main__':\n","    #import argparse\n","    #parser = argparse.ArgumentParser()\n","    #parser.add_argument('out_folder', type=str, help='where to store scraped images.')\n","    #parser.add_argument('--dictionary', type=str, help='class2url dictionary path')\n","    #args = parser.parse_args()\n","\n","    print('Scraping DermNet for URLs.')\n","    #if args.dictionary:\n","    #    with open(args.dictionary, 'rb') as fp:\n","    #        image_dict = cPickle.load(fp)\n","    #else:\n","    image_dict = genClass2URL()\n","\n","    n_images = 0\n","    for klass, images in image_dict.iteritems():\n","        n_images += len(images)\n","\n","    n_downloaded = 0\n","\n","    with open(os.path.join('/content/drive/My Drive/SomethingKewl/', 'backup.pkl'),'wb') as fp:\n","        cPickle.dump(image_dict, fp)\n","    print('Dumped dictionary of URLs to current directory.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e__3ccVQ0nXu","colab_type":"code","colab":{}},"source":["   # we will now download each image\n","    for klass, images in image_dict.items():\n","        # create class folders, if it doesn't exist\n","        class_path = os.path.join('/content/drive/My Drive/SomethingKewl/', klass)\n","        if not os.path.exists(class_path):\n","            os.mkdir(class_path)\n","\n","        for image in images:\n","            image_name = os.path.basename(image)\n","            file_name = os.path.join(class_path, image_name)\n","            # download image\n","            try:\n","                f = urlopen(image).read()\n","                open(file_name, 'wb').write(f)\n","                n_downloaded += 1\n","                print('Downloaded [{}/{}] images.'.format(n_downloaded, n_images))\n","            except :\n","                continue"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TL4Cwgob0oCo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}